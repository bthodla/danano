{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"github_analytics.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"-f5Wr83AvUlt","colab_type":"code","outputId":"a9a22bbb-5497-448d-9276-17b3d191d21f","executionInfo":{"status":"ok","timestamp":1549990406717,"user_tz":300,"elapsed":1205,"user":{"displayName":"Bhasker Thodla","photoUrl":"https://lh6.googleusercontent.com/-jSTbjwO-bPI/AAAAAAAAAAI/AAAAAAAAq7Y/AlhhsjAX-54/s64/photo.jpg","userId":"13170490952005779844"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"cell_type":"code","source":["# Install pySpark\n","# This only needs to be done once per notebook (?)\n","# apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","# !wget -q http://apache.claz.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n","# !tar xf spark-2.4.0-bin-hadoop2.7.tgz\n","# !ls -al /usr/lib/jvm/java-8-openjdk-amd64\n","# !pip install -q findspark"],"execution_count":0,"outputs":[{"output_type":"stream","text":["total 28\n","drwxr-xr-x 7 root root 4096 Feb  8 17:10 .\n","drwxr-xr-x 4 root root 4096 Feb  8 17:08 ..\n","lrwxrwxrwx 1 root root   22 Jan 14 21:02 ASSEMBLY_EXCEPTION -> jre/ASSEMBLY_EXCEPTION\n","drwxr-xr-x 2 root root 4096 Feb  8 17:10 bin\n","lrwxrwxrwx 1 root root   41 Jan 14 21:02 docs -> ../../../share/doc/openjdk-8-jre-headless\n","drwxr-xr-x 3 root root 4096 Feb  8 17:10 include\n","drwxr-xr-x 5 root root 4096 Feb  8 17:08 jre\n","drwxr-xr-x 3 root root 4096 Feb  8 17:10 lib\n","drwxr-xr-x 4 root root 4096 Feb  8 17:08 man\n","lrwxrwxrwx 1 root root   20 Jan 14 21:02 src.zip -> ../openjdk-8/src.zip\n","lrwxrwxrwx 1 root root   22 Jan 14 21:02 THIRD_PARTY_README -> jre/THIRD_PARTY_README\n"],"name":"stdout"}]},{"metadata":{"id":"xijKLL4hn4LP","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\""],"execution_count":0,"outputs":[]},{"metadata":{"id":"9aWpnXHcoSg5","colab_type":"code","colab":{}},"cell_type":"code","source":["import findspark\n","findspark.init()\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7uHGknnNv3__","colab_type":"code","outputId":"6ec2a34a-0d01-4505-94cb-4003f138e2be","executionInfo":{"status":"ok","timestamp":1549916903512,"user_tz":300,"elapsed":26971,"user":{"displayName":"Bhasker Thodla","photoUrl":"https://lh6.googleusercontent.com/-jSTbjwO-bPI/AAAAAAAAAAI/AAAAAAAAq7Y/AlhhsjAX-54/s64/photo.jpg","userId":"13170490952005779844"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["# Install the PyDrive wrapper & import libraries.\n","# This only needs to be done once per notebook.\n","# !pip install -U -q PyDrive\n","# from pydrive.auth import GoogleAuth\n","# from pydrive.drive import GoogleDrive\n","# from google.colab import auth\n","# from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","# auth.authenticate_user()\n","# gauth = GoogleAuth()\n","# gauth.credentials = GoogleCredentials.get_application_default()\n","# drive = GoogleDrive(gauth)\n","\n","# Download a file based on its file ID.\n","#\n","# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n","# file_id = 'REPLACE_WITH_YOUR_FILE_ID'\n","# downloaded = drive.CreateFile({'id': file_id})\n","# print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","print('Authenticated')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Authenticated\n"],"name":"stdout"}]},{"metadata":{"id":"6Tb_nZRRcu_M","colab_type":"code","outputId":"9e683897-1733-42d9-fa56-e8e084efb94b","executionInfo":{"status":"ok","timestamp":1549916910032,"user_tz":300,"elapsed":128,"user":{"displayName":"Bhasker Thodla","photoUrl":"https://lh6.googleusercontent.com/-jSTbjwO-bPI/AAAAAAAAAAI/AAAAAAAAq7Y/AlhhsjAX-54/s64/photo.jpg","userId":"13170490952005779844"}},"colab":{"base_uri":"https://localhost:8080/","height":80}},"cell_type":"code","source":["%%bigquery --project gbqthodlas df\n","\n","SELECT count(*) FROM `publicdata.samples.github_timeline`"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>f0_</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6219749</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       f0_\n","0  6219749"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"QoSJGq5ef25H","colab_type":"code","colab":{}},"cell_type":"code","source":["project_id = 'gbqthodlas'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O6hZ4Xrhf7Uy","colab_type":"code","outputId":"6bb00a9d-5e6c-4eb6-ec40-59bb07bff25f","executionInfo":{"status":"ok","timestamp":1549916940576,"user_tz":300,"elapsed":10330,"user":{"displayName":"Bhasker Thodla","photoUrl":"https://lh6.googleusercontent.com/-jSTbjwO-bPI/AAAAAAAAAAI/AAAAAAAAq7Y/AlhhsjAX-54/s64/photo.jpg","userId":"13170490952005779844"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.cloud import bigquery\n","\n","client = bigquery.Client(project=project_id)\n","\n","sample_count = 2000\n","row_count = client.query('''\n","  SELECT \n","    COUNT(*) as total\n","  FROM `publicdata.samples.github_timeline`''').to_dataframe().total[0]\n","\n","df = client.query('''\n","  SELECT\n","    *\n","  FROM\n","    `publicdata.samples.github_timeline`\n","  WHERE RAND() < %d/%d\n","''' % (sample_count, row_count)).to_dataframe()\n","\n","print('Full dataset has %d rows' % row_count)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Full dataset has 6219749 rows\n"],"name":"stdout"}]},{"metadata":{"id":"v6ghIjTDgYPn","colab_type":"code","outputId":"cfe3bee2-0114-4ac4-d72a-f7fc5d21b968","executionInfo":{"status":"ok","timestamp":1549990533621,"user_tz":300,"elapsed":385,"user":{"displayName":"Bhasker Thodla","photoUrl":"https://lh6.googleusercontent.com/-jSTbjwO-bPI/AAAAAAAAAAI/AAAAAAAAq7Y/AlhhsjAX-54/s64/photo.jpg","userId":"13170490952005779844"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"cell_type":"code","source":["# list(df)\n","df.head(10)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(hello='world'),\n"," Row(hello='world'),\n"," Row(hello='world'),\n"," Row(hello='world'),\n"," Row(hello='world'),\n"," Row(hello='world'),\n"," Row(hello='world'),\n"," Row(hello='world'),\n"," Row(hello='world'),\n"," Row(hello='world')]"]},"metadata":{"tags":[]},"execution_count":29}]},{"metadata":{"id":"bf6-11cDVK5L","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"J8ykfFBrTkJE","colab_type":"code","outputId":"9608ead4-329d-4885-d7fa-66b8bfd151a4","executionInfo":{"status":"error","timestamp":1549993468885,"user_tz":300,"elapsed":1149,"user":{"displayName":"Bhasker Thodla","photoUrl":"https://lh6.googleusercontent.com/-jSTbjwO-bPI/AAAAAAAAAAI/AAAAAAAAq7Y/AlhhsjAX-54/s64/photo.jpg","userId":"13170490952005779844"}},"colab":{"base_uri":"https://localhost:8080/","height":2322}},"cell_type":"code","source":["from pyspark import SparkContext\n","from pyspark.sql import SQLContext\n","from pyspark.sql.types import *\n","\n","\n","if __name__ == \"__main__\":\n","    sc = SparkContext(appName=\"CSV2Parquet\")\n","    sqlContext = SQLContext(sc)\n","    \n","    schema = StructType([\n","            StructField(\"customer_id\", IntegerType(), True),\n","            StructField(\"customer_name\", StringType(), True),\n","            StructField(\"category\", StringType(), True)])\n","    \n","    rdd = sc.textFile(\"\\\\\\\\lordabbett.com\\\\nas\\\\EnterprisePlatforms\\\\Qlik\\\\Hackathon\\\\qs_test_data\\\\customer.csv\").map(lambda line: line.split(\",\"))\n","    df = sqlContext.createDataFrame(rdd, schema)\n","    df.write.parquet('\\\\\\\\lordabbett.com\\\\nas\\\\EnterprisePlatforms\\\\Qlik\\\\Hackathon\\\\qs_test_data\\\\customer-parquet')"],"execution_count":34,"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-fde2f669bfa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\\\\\\lordabbett.com\\\\nas\\\\EnterprisePlatforms\\\\Qlik\\\\Hackathon\\\\qs_test_data\\\\customer.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\\\\\\lordabbett.com\\\\nas\\\\EnterprisePlatforms\\\\Qlik\\\\Hackathon\\\\qs_test_data\\\\customer-parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o86.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:196)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:276)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:270)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:557)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Illegal file pattern: Illegal/unsupported escape sequence near index 22\n\\\\lordabbett\\.com\\nas\\EnterprisePlatforms\\Qlik\\Hackathon\\qs_test_data\\customer\\.csv\n                      ^\n\tat org.apache.hadoop.fs.GlobFilter.init(GlobFilter.java:71)\n\tat org.apache.hadoop.fs.GlobFilter.<init>(GlobFilter.java:50)\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:192)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:259)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:55)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:159)\n\t... 33 more\nCaused by: java.util.regex.PatternSyntaxException: Illegal/unsupported escape sequence near index 22\n\\\\lordabbett\\.com\\nas\\EnterprisePlatforms\\Qlik\\Hackathon\\qs_test_data\\customer\\.csv\n                      ^\n\tat java.util.regex.Pattern.error(Pattern.java:1957)\n\tat java.util.regex.Pattern.escape(Pattern.java:2473)\n\tat java.util.regex.Pattern.atom(Pattern.java:2200)\n\tat java.util.regex.Pattern.sequence(Pattern.java:2081)\n\tat java.util.regex.Pattern.expr(Pattern.java:1998)\n\tat java.util.regex.Pattern.compile(Pattern.java:1698)\n\tat java.util.regex.Pattern.<init>(Pattern.java:1351)\n\tat java.util.regex.Pattern.compile(Pattern.java:1028)\n\tat org.apache.hadoop.fs.GlobPattern.set(GlobPattern.java:156)\n\tat org.apache.hadoop.fs.GlobPattern.<init>(GlobPattern.java:42)\n\tat org.apache.hadoop.fs.GlobFilter.init(GlobFilter.java:67)\n\t... 75 more\n"]}]}]}